---
title: "Confusion Matrix"

format:
  html:
    code-fold: true
---


##  Objective

This tutorial demonstrates how to use the **confusion matrix** to evaluate the performance of a classification model in Python using `scikit-learn`. 
We will:

- Load and prepare the Iris dataset
- Build a Decision Tree classifier
- Generate predictions
- Calculate classification metrics (accuracy, precision, recall, F1 score)
- Visualize the confusion matrix
- Print a detailed classification report

---

## Import required libraries

We begin by importing the necessary libraries for data loading, modeling, evaluation, and visualization.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report

```
## Load and inspect the Iris dataset
We load the classic Iris dataset from scikit-learn. This dataset contains three flower classes: setosa, versicolor, and virginica

```{python}
#| label: :loading_iris
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names

```
## Select only two classes: versicolor and virginica
To simplify the binary classification problem, we filter out only the samples labeled as 1 (versicolor) and 2 (virginica).

```{python}
mask = (y == 1) | (y == 2)
X = X[mask]
y = y[mask]
```

## Split the data into training and test sets
We split the dataset into 70% training and 30% test sets using train_test_split.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

```

## Train a Decision Tree Classifier
We use a simple Decision Tree model to classify the flower samples.

```{python}
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
```

## Make predictions
After training the model, we predict the classes for the test set.

```{python}
y_pred = model.predict(X_test)

```

## Calculate evaluation metrics
Now we calculate common classification metrics:

- Accuracy: Overall correctness

- Precision: How many selected items are relevant

- Recall: How many relevant items are selected

- F1 Score: Harmonic mean of precision and recall
```{python}
cm = confusion_matrix(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1 Score:", f1)
```

## Visualize the Confusion Matrix
A confusion matrix  @cm is a table that describes the performance of a classification model. Each row represents the instances in an actual class, while each column represents the instances in a predicted class.
```{python}
#| label: cm
#| tbl-cap: "Confusion matrix"
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=["versicolor", "virginica"],
            yticklabels=["versicolor", "virginica"])
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

```

## Detailed Classification Report
We use classification_report() to display precision, recall, F1-score, and support (number of true instances) for each class.
```{python}
print(classification_report(y_test, y_pred, target_names=["versicolor", "virginica"]))

```

## Summary
The confusion matrix and related metrics provide a comprehensive view of model performance. In binary classification problems like this one, it's crucial to go beyond simple accuracy and analyze precision, recall, and F1-score, especially if the classes are imbalanced.